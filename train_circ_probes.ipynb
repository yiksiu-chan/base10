{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params:\n",
      "\n",
      "{'model_name': 'Qwen/Qwen2-0.5B', 'use_4bit': False, 'epochs': 10000, 'lr': 0.0005, 'numbers': 2000, 'batch_size': 2000, 'exclude': 'random', 'exclude_count': 200, 'positions': 1, 'shuffle': True, 'bases': [10, 11], 'start_layer': 0, 'bias': False}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60f1a8ba23de409397496d8261ee3182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a351360184d486a96446aaf4b68eeae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "190a79c52d074305a5648cfa512dc6ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56c0496ea54747e6953561800c964032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "630dbb1ea19241ccbbf294b6fc2ebcb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/661 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7239de1983dc40938fe7cb4dd7be2f57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/988M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6570b42ff5c4d3ea877247aa6252e1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/138 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device of model is cuda:0\n"
     ]
    }
   ],
   "source": [
    "from circular_probe import train_circular_probe\n",
    "import numpy\n",
    "from tqdm import tqdm\n",
    "from general_ps_utils import ModelAndTokenizer\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "params = {\n",
    "    'model_name': \"Qwen/Qwen2-0.5B\", #\"google/gemma-2-2b\", #\"meta-llama/Meta-Llama-3-8B\", #\"mistralai/Mistral-7B-v0.1\",\n",
    "    'use_4bit': False,\n",
    "    'epochs': 10_000,\n",
    "    'lr': 0.0005,\n",
    "    'numbers': 2000,\n",
    "    'batch_size': 2000,\n",
    "    'exclude': 'random', # numbers to exclude from training set\n",
    "    'exclude_count': 200,\n",
    "    'positions': 1,\n",
    "    'shuffle': True,\n",
    "    'bases': [10,11],\n",
    "    'start_layer': 0,\n",
    "    'bias': False\n",
    "}\n",
    "\n",
    "\n",
    "print(f\"Params:\\n\\n{params}\")\n",
    "\n",
    "if params['exclude'] == 'random':\n",
    "    params['exclude'] = numpy.random.choice(params['numbers'], params['exclude_count'], replace=False)\n",
    "\n",
    "mt = ModelAndTokenizer(\n",
    "    model_name=params['model_name'],\n",
    "    use_4bit=params['use_4bit'],\n",
    "    device='cuda'\n",
    ")\n",
    "\n",
    "tokenizer = mt.tokenizer\n",
    "num_to_hidden = dict()\n",
    "\n",
    "# move device to cuda because for some reason it is not\n",
    "mt.model.to('cuda')\n",
    "\n",
    "print(f\"device of model is {mt.model.device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(params['numbers']), delay=120):\n",
    "    text_for_embeddings = \"\"\n",
    "\n",
    "    for _ in range(params['positions']):\n",
    "        text_for_embeddings += str(i) + \" \"\n",
    "    text_for_embeddings = text_for_embeddings[:-1]\n",
    "\n",
    "    x = tokenizer.encode(text_for_embeddings, return_tensors='pt')\n",
    "    x = x.to(mt.device)\n",
    "    hidden_states = mt.model(x, output_hidden_states=True).hidden_states\n",
    "\n",
    "    num_to_hidden[i] = hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training cyclic probe on basis: 10\n",
      "896\n",
      "X[0].shape=torch.Size([896])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1396843/2776300437.py:23: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, df_delta], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: 0, Accuracy: 0.0\n",
      "896\n",
      "X[0].shape=torch.Size([896])\n",
      "Layer: 1, Accuracy: 0.92\n",
      "896\n",
      "X[0].shape=torch.Size([896])\n",
      "Layer: 2, Accuracy: 0.885\n",
      "896\n",
      "X[0].shape=torch.Size([896])\n",
      "Layer: 3, Accuracy: 0.915\n",
      "896\n",
      "X[0].shape=torch.Size([896])\n",
      "Layer: 4, Accuracy: 0.915\n",
      "896\n",
      "X[0].shape=torch.Size([896])\n",
      "Layer: 5, Accuracy: 0.94\n",
      "896\n",
      "X[0].shape=torch.Size([896])\n",
      "Layer: 6, Accuracy: 0.855\n",
      "896\n",
      "X[0].shape=torch.Size([896])\n",
      "Layer: 7, Accuracy: 0.845\n",
      "896\n",
      "X[0].shape=torch.Size([896])\n",
      "Layer: 8, Accuracy: 0.795\n",
      "896\n",
      "X[0].shape=torch.Size([896])\n",
      "Layer: 9, Accuracy: 0.915\n",
      "896\n",
      "X[0].shape=torch.Size([896])\n",
      "Layer: 10, Accuracy: 0.905\n",
      "896\n",
      "X[0].shape=torch.Size([896])\n",
      "Layer: 11, Accuracy: 0.885\n",
      "896\n",
      "X[0].shape=torch.Size([896])\n",
      "Layer: 12, Accuracy: 0.835\n",
      "896\n",
      "X[0].shape=torch.Size([896])\n",
      "Layer: 13, Accuracy: 0.84\n",
      "896\n",
      "X[0].shape=torch.Size([896])\n",
      "Layer: 14, Accuracy: 0.775\n",
      "896\n",
      "X[0].shape=torch.Size([896])\n",
      "Layer: 15, Accuracy: 0.735\n",
      "896\n",
      "X[0].shape=torch.Size([896])\n",
      "Layer: 16, Accuracy: 0.775\n",
      "896\n",
      "X[0].shape=torch.Size([896])\n",
      "Layer: 17, Accuracy: 0.74\n",
      "896\n",
      "X[0].shape=torch.Size([896])\n",
      "Layer: 18, Accuracy: 0.755\n",
      "896\n",
      "X[0].shape=torch.Size([896])\n",
      "Layer: 19, Accuracy: 0.705\n",
      "896\n",
      "X[0].shape=torch.Size([896])\n",
      "Layer: 20, Accuracy: 0.715\n",
      "896\n",
      "X[0].shape=torch.Size([896])\n",
      "Layer: 21, Accuracy: 0.675\n",
      "896\n",
      "X[0].shape=torch.Size([896])\n",
      "Layer: 22, Accuracy: 0.62\n",
      "896\n",
      "X[0].shape=torch.Size([896])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████████████████████████████████████████████████████████████████▌                                                                | 1/2 [03:27<03:27, 207.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: 23, Accuracy: 0.685\n",
      "Training cyclic probe on basis: 11\n",
      "896\n",
      "X[0].shape=torch.Size([896])\n",
      "Layer: 0, Accuracy: 0.0\n",
      "896\n",
      "X[0].shape=torch.Size([896])\n",
      "Layer: 1, Accuracy: 0.01\n",
      "896\n",
      "X[0].shape=torch.Size([896])\n",
      "Layer: 2, Accuracy: 0.065\n",
      "896\n",
      "X[0].shape=torch.Size([896])\n",
      "Layer: 3, Accuracy: 0.025\n",
      "896\n",
      "X[0].shape=torch.Size([896])\n",
      "Layer: 4, Accuracy: 0.04\n",
      "896\n",
      "X[0].shape=torch.Size([896])\n",
      "Layer: 5, Accuracy: 0.06\n",
      "896\n",
      "X[0].shape=torch.Size([896])\n",
      "Layer: 6, Accuracy: 0.03\n",
      "896\n",
      "X[0].shape=torch.Size([896])\n",
      "Layer: 7, Accuracy: 0.055\n",
      "896\n",
      "X[0].shape=torch.Size([896])\n",
      "Layer: 8, Accuracy: 0.025\n",
      "896\n",
      "X[0].shape=torch.Size([896])\n",
      "Layer: 9, Accuracy: 0.04\n",
      "896\n",
      "X[0].shape=torch.Size([896])\n",
      "Layer: 10, Accuracy: 0.05\n",
      "896\n",
      "X[0].shape=torch.Size([896])\n",
      "Layer: 11, Accuracy: 0.055\n",
      "896\n",
      "X[0].shape=torch.Size([896])\n",
      "Layer: 12, Accuracy: 0.035\n",
      "896\n",
      "X[0].shape=torch.Size([896])\n",
      "Layer: 13, Accuracy: 0.04\n",
      "896\n",
      "X[0].shape=torch.Size([896])\n",
      "Layer: 14, Accuracy: 0.04\n",
      "896\n",
      "X[0].shape=torch.Size([896])\n",
      "Layer: 15, Accuracy: 0.05\n",
      "896\n",
      "X[0].shape=torch.Size([896])\n",
      "Layer: 16, Accuracy: 0.025\n",
      "896\n",
      "X[0].shape=torch.Size([896])\n",
      "Layer: 17, Accuracy: 0.035\n",
      "896\n",
      "X[0].shape=torch.Size([896])\n"
     ]
    }
   ],
   "source": [
    "# need to average over all layers per basis\n",
    "import pandas as pd\n",
    "\n",
    "# set up df\n",
    "df = pd.DataFrame(columns=['layer', 'basis', 'accuracy'])\n",
    "\n",
    "\n",
    "basis_acc = dict()\n",
    "basis_acc_max = dict()\n",
    "basis_acc_argmax = dict()\n",
    "for basis in tqdm(params['bases']):\n",
    "    print(f\"Training cyclic probe on basis: {basis}\")\n",
    "    params['basis'] = basis\n",
    "    layer_acc = []\n",
    "    for layer in range(params['start_layer'], mt.num_layers):\n",
    "        params['layers'] = [layer]\n",
    "        acc, circular_probe = train_circular_probe(params, mt, num_to_hidden)\n",
    "        print(f\"Layer: {layer}, Accuracy: {acc}\")\n",
    "        layer_acc.append(acc)\n",
    "\n",
    "        # add to df\n",
    "        df_delta = pd.DataFrame([[layer, basis, acc]], columns=['layer', 'basis', 'accuracy'])\n",
    "        df = pd.concat([df, df_delta], ignore_index=True)\n",
    "\n",
    "    basis_acc[basis] = numpy.mean(layer_acc)\n",
    "    basis_acc_max[basis] = numpy.max(layer_acc)\n",
    "    basis_acc_argmax[basis] = numpy.argmax(layer_acc)\n",
    "\n",
    "# round all acc numbers to 3 decimal places\n",
    "basis_acc = {k: round(v, 3) for k, v in basis_acc.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: google/gemma-2-2b\n",
      "Number of numbers: 2000\n",
      "Epochs: 10000\n",
      "Start layer: 0\n",
      "Basis\tAccuracy\n",
      "10\t0.626\n",
      "11\t0.075\n",
      "10\t0.995 (layer: 1)\n",
      "11\t0.255 (layer: 3)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model: {params['model_name']}\")\n",
    "print(f\"Number of numbers: {params['numbers']}\")\n",
    "print(f\"Epochs: {params['epochs']}\")\n",
    "print(f\"Start layer: {params['start_layer']}\")\n",
    "\n",
    "# print results as a table\n",
    "print(\"Basis\\tAccuracy\")\n",
    "\n",
    "for basis, acc in basis_acc.items():\n",
    "    print(f\"{basis}\\t{acc}\")\n",
    "\n",
    "for basis, acc in basis_acc_max.items():\n",
    "    print(f\"{basis}\\t{acc} (layer: {basis_acc_argmax[basis]})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
